{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Section 0: Preamble\n",
    "===================\n",
    "\n",
    "These lectures assume the current working directory is `/path/to/autofit_workspace/` on your hard-disk (or in Binder).\n",
    "This is so that it can:\n",
    "\n",
    " - Load configuration settings from config files in the `autofit_workspace/config` folder.\n",
    " - Load example data from the `autofit_workspace/dataset` folder.\n",
    " - Output the results of models fits to your hard-disk to the `autofit/output` folder.\n",
    "\n",
    "If you don't have an autofit_workspace (perhaps you cloned / forked the **PyAutoLens** GitHub repository?) you can\n",
    "download it here:\n",
    "\n",
    " https://github.com/Jammy2211/autofit_workspace\n",
    "\n",
    "At the top of every lecture notebook, you'll see the following cell. This cell uses the project `pyprojroot` to\n",
    "locate the path to the workspace on your computer and use it to set the working directory of the notebook.\n",
    "\n",
    "The imports required for this lecture are also performed below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import autofit as af\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from pyprojroot import here\n",
    "workspace_path = str(here())\n",
    "%cd $workspace_path\n",
    "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
    "\n",
    "import os\n",
    "from os import path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Section 1: Model Composition\n",
    "============================\n",
    "\n",
    "First, we'll compose a simple model and show how **PyAutoFit** allows us to generate instances of the model, which\n",
    "will allow us to fit the model to data later on.\n",
    "\n",
    "__Data__\n",
    "\n",
    "Throughout these tutorials we will fit noisy 1D data containing a signal, where the signal was generated using a\n",
    "Gaussian.\n",
    "\n",
    "These are loaded from .json files, where:\n",
    "\n",
    " - The data is a 1D numpy array of values corresponding to the observed counts of the Gaussian.\n",
    " - The noise-map corresponds to the expected noise value in every data point.\n",
    "\n",
    "These datasets were created using the following script:\n",
    "\n",
    "https://github.com/Jammy2211/autofit_workspace/blob/release/scripts/simulators/simulators.py\n",
    "\n",
    "Feel free to check it out!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1\")\n",
    "\n",
    "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
    "noise_map = af.util.numpy_array_from_json(\n",
    "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets plot the `Gaussian` via Matplotlib.\n",
    "\n",
    "The 1D signal is observed on a line of uniformly spaced xvalues, which we'll compute using the shape of the data and\n",
    "plot as the x-axis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xvalues = np.arange(data.shape[0])\n",
    "plt.plot(xvalues, data, color=\"k\")\n",
    "plt.title(\"1D Gaussian dataset.\")\n",
    "plt.xlabel(\"x values of profile\")\n",
    "plt.ylabel(\"Signal Normalization\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also plot its `noise_map` (which in this example are all constant values) as a standalone 1D plot or\n",
    "as error bars on the `data`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(xvalues, noise_map, color=\"k\")\n",
    "plt.title(\"Noise-map\")\n",
    "plt.xlabel(\"x values of noise-map\")\n",
    "plt.ylabel(\"Noise-map value (Root mean square error)\")\n",
    "plt.show()\n",
    "\n",
    "plt.errorbar(\n",
    "    xvalues, data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
    ")\n",
    "plt.title(\"1D Gaussian dataset with errors from the noise-map.\")\n",
    "plt.xlabel(\"x values of profile\")\n",
    "plt.ylabel(\"Profile Normalization\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Model Parameterization__\n",
    "\n",
    "We now wish to define a model that can fit the signal in this data.\n",
    "\n",
    "What model could fit this data? The obvious choice is a one-dimensional `Gaussian` defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "g(x, I, \\sigma) = \\frac{N}{\\sigma\\sqrt{2\\pi}} \\exp{(-0.5 (x / \\sigma)^2)}\n",
    "\\end{equation*}\n",
    "\n",
    "Where:\n",
    "\n",
    "x - Is an x-axis coordinate where the `Gaussian` is evaluated.\n",
    "N - Describes the overall normalization of the Gaussian.\n",
    "$\\sigma$ - Describes the size of the Gaussian.\n",
    "\n",
    "This simple equation describes our model, a 1D `Gaussian`, and it has 3 parameters, $(x, N, \\sigma)$. Using different\n",
    "values of these 3 parameters we can create a realization of any 1D Gaussian.\n",
    "\n",
    "__Model Composition__\n",
    "\n",
    "We now compose the 1D `Gaussian` above as a model in **PyAutoFit**.\n",
    "\n",
    "First, we write a Python class as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Gaussian:\n",
    "    def __init__(\n",
    "        self,\n",
    "        centre=30.0,  # <- **PyAutoFit** recognises these constructor arguments\n",
    "        normalization=1.0,  # <- are the Gaussian`s model parameters.\n",
    "        sigma=5.0,\n",
    "    ):\n",
    "        self.centre = centre\n",
    "        self.normalization = normalization\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def profile_from_xvalues(self, xvalues : np.ndarray):\n",
    "        \"\"\"\n",
    "        Calculate the 1D Gaussian profile on a line of Cartesian x coordinates.\n",
    "\n",
    "        The input xvalues are translated to a coordinate system centred on the Gaussian, using its centre.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        xvalues\n",
    "            The x coordinates in the original reference frame of the data.\n",
    "        \"\"\"\n",
    "        transformed_xvalues = np.subtract(xvalues, self.centre)\n",
    "        return np.multiply(\n",
    "            np.divide(self.normalization, self.sigma * np.sqrt(2.0 * np.pi)),\n",
    "            np.exp(-0.5 * np.square(np.divide(transformed_xvalues, self.sigma))),\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The format of this Python class defines how **PyAutoFit** will compose it as a model, where:\n",
    "\n",
    "- The name of the class is the name of the model component, in this case, \"Gaussian\".\n",
    "\n",
    "- The input arguments of the constructor are the parameters of the model, which we will vary when we fit it to our\n",
    " data. In this case, the free parameters are `centre`, `normalization` and `sigma`.\n",
    "\n",
    "- The default values of the input arguments tell **PyAutoFit** whether a parameter is a single-valued `float` or a\n",
    "  multi-valued `tuple`. For the `Gaussian` class, no input parameters are a tuple and we will show an example of a\n",
    "  tuple input in a later tutorial).\n",
    "\n",
    "- It includes functions associated with that model component, for example the `profile_from_xvalues` function, which\n",
    "  allows an instance of a `Gaussian` to create its 1D representation as a NumPy array.\n",
    "\n",
    "To compose the model using the `Gaussian` class above we use the **PyAutoFit** `Model` object."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = af.Model(Gaussian)\n",
    "print(\"Model `Gaussian` object: \\n\")\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To use a model, we have to set up its parameters as priors. We'll cover priors in section 3, so for now just ignore\n",
    "the code below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'af' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-091508889933>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcentre\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0maf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mUniformPrior\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlower_limit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mupper_limit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m100.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnormalization\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0maf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mUniformPrior\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlower_limit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mupper_limit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m100.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msigma\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0maf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mUniformPrior\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlower_limit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mupper_limit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m100.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'af' is not defined"
     ]
    }
   ],
   "source": [
    "model.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
    "model.normalization = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
    "model.sigma = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can inspect the model to note that it indeed has a total of 3 parameters (why this is called `prior_count` will be\n",
    "explained in section 3):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(model.prior_count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Model Mapping__\n",
    "\n",
    "At the core of **PyAutoFit** is how it map Python classes that are set up via the `Model` object to instances of\n",
    "that Python classes, where the values of its parameters are set during this mapping.\n",
    "\n",
    "For example, we below create an `instance` of the model, by mapping a list of physical values of each parameter as\n",
    "follows."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "instance = model.instance_from_vector(vector=[30.0, 2.0, 3.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is an instance of the `Gaussian` class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Model Instance: \\n\")\n",
    "print(instance)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It has the parameters of the `Gaussian` with the values input above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Instance Parameters \\n\")\n",
    "print(\"x = \", instance.centre)\n",
    "print(\"normalization = \", instance.normalization)\n",
    "print(\"sigma = \", instance.sigma)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use the functions associated with this class, specifically the `profile_from_xvalues` function, to create a\n",
    "realization of the Gaussian and plot it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "realization = instance.profile_from_xvalues(xvalues=xvalues)\n",
    "\n",
    "plt.plot(xvalues, realization, color=\"r\")\n",
    "plt.title(\"1D Gaussian Realization.\")\n",
    "plt.xlabel(\"x values of profile\")\n",
    "plt.ylabel(\"Profile Normalization\")\n",
    "plt.show()\n",
    "plt.clf()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Discussion__\n",
    "\n",
    "Whilst the simple example above is informative, it may be somewhat underwhelming. Afterall, how difficult would it\n",
    "have been to write Python code that defines this `Gaussian` with those values yourself? Why bother setting up the\n",
    "`Gaussian` as a `Model` and using the `model.instance_from_vector` command when you could of just set up an instance\n",
    "of the Gaussian by passing it the parameters manually yourself, e.g.:\n",
    "\n",
    "`instance = Gaussian(centre=30.0, normalization=2.0, sigma=3.0)`\n",
    "\n",
    "The reason, is because model composition and mapping get very complicated very quickly. In section 3, we'll introduce\n",
    "priors, which also need to be fully for when performing model mapping. Later tutorials will compose models with 10+\n",
    "parameters, which require high levels of customization in their parameterization. At the end of chapter 1 we'll use\n",
    "**PyAutoFit** to build multi-level models from hierarchies of Python classes that potentially comprise hundreds of\n",
    "parameters!\n",
    "\n",
    "Therefore, things might seem somewhat and unnecessary right now, but the tools we're covering now will enable very\n",
    "complex models to be composed, mapped and fitted by the end of this chapter!\n",
    "\n",
    "__Wrap Up__\n",
    "\n",
    "In this section, we introduced how to parameterize and compose a model and define priors for each of its parameters.\n",
    "This used Python classes and allowed us to map input values to instances of this Python class. Nothing we've introduced\n",
    "so far was particularly remarkable, but it has presented us with the core interface we'll use to do advanced model\n",
    "fitting in later tutorials.\n",
    "\n",
    "Finally, quickly think about a model you might want to fit. How would you write it as a Python class using the format\n",
    "above? What are the free parameters of you model? Are there multiple model components you are going to want to fit to\n",
    "your data data?\n",
    "\n",
    "If you decide to add a new model-component to the `autofit_workspace` specific to your model-fitting task, first\n",
    "checkout the following script, which explains how to set up the **PyAutoFit** configuration files associated with\n",
    "your model.\n",
    "\n",
    "`autofit_workspace/notebooks/overview/new_model_component/new_model_component.ipynb`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Section 2: Model Fitting\n",
    "========================\n",
    "\n",
    "In this section, we'll fit the 1D `Gaussian` from the previous tutorial to the data containing that Gaussian, and\n",
    "learn how to quantify the goodness-of-fit.\n",
    "\n",
    "The shape of the data gives us its xvalues, the x coordinates we evaluate our model 1D `Gaussian` on."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xvalues = np.arange(data.shape[0])\n",
    "print(xvalues)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets plot the data and noise-map we're going to fit again, just as a reminder of what it looks like."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.errorbar(\n",
    "    xvalues, data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
    ")\n",
    "plt.title(\"1D Gaussian dataset.\")\n",
    "plt.xlabel(\"x values of profile\")\n",
    "plt.ylabel(\"Profile Normalization\")\n",
    "plt.show()\n",
    "plt.clf()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Model Data__\n",
    "\n",
    "So, how do we actually go about fitting our `Gaussian` model to this data? First, we need to be able to generate\n",
    "an image of our 1D `Gaussian` model. For this, we can use the `Gaussian` we defined in section 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gaussian = model.instance_from_vector(vector=[60.0, 20.0, 15.0])\n",
    "\n",
    "model_data = gaussian.profile_from_xvalues(xvalues=xvalues)\n",
    "\n",
    "plt.plot(xvalues, model_data, color=\"r\")\n",
    "plt.title(\"1D Gaussian model.\")\n",
    "plt.xlabel(\"x values of profile\")\n",
    "plt.ylabel(\"Profile Normalization\")\n",
    "plt.show()\n",
    "plt.clf()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is often more informative to plot the `data` and `model_data` on the same plot for comparison."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.errorbar(\n",
    "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
    ")\n",
    "plt.plot(xvalues, model_data, color=\"r\")\n",
    "plt.title(\"Model-data fit to 1D Gaussian data.\")\n",
    "plt.xlabel(\"x values of profile\")\n",
    "plt.ylabel(\"Profile normalization\")\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Residuals__\n",
    "\n",
    "Different values of `centre`, `normalization` and `sigma` change the `Gaussian`'s appearance, have a go at editing some\n",
    "of the values input into `instance_from_vector()` above to see this behaviour.\n",
    "\n",
    "Lets recap. We've defined a model which is a 1D `Gaussian` and given a set of parameters for that model $(x, N, \\sigma)$\n",
    "we can create `model_data` of the `Gaussian`. We have some data of a 1D `Gaussian` we want to fit this model with,\n",
    "so as to determine the values of $(x, N, \\sigma)$ from which it was created. So how do we do that?\n",
    "\n",
    "Simple, we take the our `data` and `model_data` and subtract the two to get a residual-map."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "residual_map = data - model_data\n",
    "plt.plot(xvalues, residual_map, color=\"r\")\n",
    "plt.title(\"Residuals of model-data fit to 1D Gaussian data.\")\n",
    "plt.xlabel(\"x values of profile\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()\n",
    "plt.clf()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Normalized Residuals__\n",
    "\n",
    "Clearly, this model is not a good fit to the data -- which was to be expected as they looked nothing alike!\n",
    "\n",
    "Next, we want to quantify how good (or bad) the fit actually was, via some goodness-of-fit measure. This measure\n",
    "needs to account for noise in the data, after all if we fit a pixel badly simply because it was very noisy we want\n",
    "our goodness-of-fit to account for that.\n",
    "\n",
    "To account for noise, we take our `residual_map` and divide it by the `noise_map`, to get the normalized residual-map."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "normalized_residual_map = residual_map / noise_map\n",
    "plt.plot(xvalues, normalized_residual_map, color=\"r\")\n",
    "plt.title(\"Normalized residuals of model-data fit to 1D Gaussian data.\")\n",
    "plt.xlabel(\"x values of profile\")\n",
    "plt.ylabel(\"Normalized Residuals\")\n",
    "plt.show()\n",
    "plt.clf()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Chi Squared__\n",
    "\n",
    "We're getting close to a goodness-of-fit measure, but there is still a problem, we have negative and positive values\n",
    "in the normalized residual-map. A value of -0.2 represents just as good of a fit as a value of 0.2, therefore we want\n",
    "them to both be the same value.\n",
    "\n",
    "Thus, we next define a chi-squared-map, which is the `normalized_residual_map` squared. This makes negative and\n",
    "positive values both positive and thus defines them on a common overall scale."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chi_squared_map = (normalized_residual_map) ** 2\n",
    "plt.plot(xvalues, chi_squared_map, color=\"r\")\n",
    "plt.title(\"Chi-Squareds of model-data fit to 1D Gaussian data.\")\n",
    "plt.xlabel(\"x values of profile\")\n",
    "plt.ylabel(\"Chi-Squareds\")\n",
    "plt.show()\n",
    "plt.clf()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Great, even when looking at a chi-squared-map its clear that our model gives a rubbish fit to the data!\n",
    "\n",
    "Finally, we want to reduce all the information in our `chi_squared_map` into a single goodness-of-fit measure. To do\n",
    "this we define the `chi_squared`, which is the sum of all values on the chi-squared-map."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chi_squared = np.sum(chi_squared_map)\n",
    "print(\"Chi-squared = \", chi_squared)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The lower our chi-squared, the fewer residuals in the fit between our model and the data and therefore the better our\n",
    "fit!\n",
    "\n",
    "__Likelihood__\n",
    "\n",
    "From the chi-squared we can then define our final goodness-of-fit measure, the `log_likelihood`, which is the\n",
    "chi-squared value multiplied by -0.5."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_likelihood = -0.5 * chi_squared\n",
    "print(\"Log Likelihood = \", log_likelihood)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Why is the log likelihood the chi-squared multiplid by -0.5? Lets not worry about. This is simply the standard\n",
    "definition of a log_likelihood in statistics (it relates to the specific case of the noise properties in our dataset\n",
    "being Gaussian). For now, just accept that this is  what a log likelihood is and if we want to fit a model to data our\n",
    "goal is to thus find the combination of model parameters that maximize the `log_likelihood`.\n",
    "\n",
    "There is a second quantity that enters the log likelihood, called the `noise_normalization`. This is the log sum of all\n",
    "noise values squared in our data. Given the noise-map is fixed, the noise_normalization retains the same value for\n",
    "all models that we fit. Nevertheless, it is good practise to include it in the log likelihood."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noise_normalization = np.sum(np.log(2 * np.pi * noise_map ** 2.0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, like the definition of a log likelihood, lets not worry about why a noise normalization is defined in this way\n",
    "or why its in our goodness-of-fit (it again relates to the noise properties of our dataset being Gaussian). Lets just\n",
    "accept for now that this is how it is in statistics.\n",
    "\n",
    "Thus, we now have the definition of a log likelihood that we'll use hereafter in all **PyAutoFit** tutorials."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
    "print(\"Log Likelihood = \", log_likelihood)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Fitting Functions__\n",
    "\n",
    "If you are familiar with model-fitting, you will have probably heard of terms like 'residuals', 'chi-squared' and\n",
    "'log_likelihood' before. These are the standard metrics by which a model-fit`s quality is quantified. They are used for\n",
    "model fitting in general, so not just when your data is 1D but when its a 2D image, 3D datacube or something else\n",
    "entirely!\n",
    "\n",
    "If you have not performed model fitting before and these terms are new to you, make sure you are clear on exactly what\n",
    "they all mean as they are at the core of all model fitting performed in **PyAutoFit** (and statistical inference in\n",
    "general)!\n",
    "\n",
    "So to recap:\n",
    "\n",
    " - We can define a model components in **PyAutoFit**, like our `Gaussian`, using Python classes that follow a certain\n",
    " format.\n",
    "\n",
    " - The model component can be set up as a `Model` and have its parameters mapped to an instance of the\n",
    " `Gaussian` class via `instance_from_vector` function.\n",
    "\n",
    " - We can use this model instance to create model-data of our `Gaussian` and compare it to data and quantify the\n",
    " goodness-of-fit via a log likelihood.\n",
    "\n",
    "Thus we have everything we need to fit our model to our data! So, how do we go about finding the best-fit model?\n",
    "That is, the model which maximizes the log likelihood.\n",
    "\n",
    "The most simple thing we can do is guess parameters, and when we guess parameters that give a good fit, guess another\n",
    "set of parameters near those values. We can then repeat this process, over and over, until we find a really good model!\n",
    "\n",
    "For our `Gaussian` this works pretty well, below I've fitted 3 different `Gaussian` models and ended up landing on\n",
    "the best-fit model (the model I used to create the dataset in the first place!).\n",
    "\n",
    "For convenience, I've create functions which compute the chi-squared-map and log likelihood of a model-fit, alongside a\n",
    "method to plot a profile, residual-map or chi-squared-map."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def chi_squared_map_from_data_and_model_data(data, noise_map, model_data):\n",
    "\n",
    "    residual_map = data - model_data\n",
    "    normalized_residual_map = residual_map / noise_map\n",
    "\n",
    "    return (normalized_residual_map) ** 2\n",
    "\n",
    "\n",
    "def log_likelihood_from_data_and_model_data(data, noise_map, model_data):\n",
    "\n",
    "    chi_squared_map = chi_squared_map_from_data_and_model_data(\n",
    "        data=data, noise_map=noise_map, model_data=model_data\n",
    "    )\n",
    "    chi_squared = sum(chi_squared_map)\n",
    "    noise_normalization = np.sum(np.log(2 * np.pi * noise_map ** 2.0))\n",
    "    log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
    "\n",
    "    return log_likelihood\n",
    "\n",
    "\n",
    "def plot_line(xvalues, line, color=\"k\", errors=None, ylabel=None):\n",
    "\n",
    "    plt.errorbar(\n",
    "        x=xvalues, y=line, yerr=errors, color=color, ecolor=\"k\", elinewidth=1, capsize=2\n",
    "    )\n",
    "    plt.title(\"Chi-Squared of model-data fit to 1D Gaussian data.\")\n",
    "    plt.xlabel(\"x values of profile\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "    plt.clf()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Guess 1__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "gaussian = model.instance_from_vector(vector=[50.0, 10.0, 5.0])\n",
    "model_data = gaussian.profile_from_xvalues(xvalues=xvalues)\n",
    "chi_squared_map = chi_squared_map_from_data_and_model_data(\n",
    "    data=data, noise_map=noise_map, model_data=model_data\n",
    ")\n",
    "plot_line(xvalues=xvalues, line=chi_squared_map, color=\"r\", ylabel=\"Chi-Squareds\")\n",
    "\n",
    "log_likelihood = log_likelihood_from_data_and_model_data(\n",
    "    data=data, noise_map=noise_map, model_data=model_data\n",
    ")\n",
    "print(\"Log Likelihood:\")\n",
    "print(log_likelihood)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Guess 2__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "gaussian = model.instance_from_vector(vector=[50.0, 25.0, 5.0])\n",
    "model_data = gaussian.profile_from_xvalues(xvalues=xvalues)\n",
    "chi_squared_map = chi_squared_map_from_data_and_model_data(\n",
    "    data=data, noise_map=noise_map, model_data=model_data\n",
    ")\n",
    "plot_line(xvalues=xvalues, line=chi_squared_map, color=\"r\", ylabel=\"Chi-Squareds\")\n",
    "\n",
    "log_likelihood = log_likelihood_from_data_and_model_data(\n",
    "    data=data, noise_map=noise_map, model_data=model_data\n",
    ")\n",
    "print(\"Log Likelihood:\")\n",
    "print(log_likelihood)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Guess 3__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "gaussian = model.instance_from_vector(vector=[50.0, 25.0, 10.0])\n",
    "model_data = gaussian.profile_from_xvalues(xvalues=xvalues)\n",
    "chi_squared_map = chi_squared_map_from_data_and_model_data(\n",
    "    data=data, noise_map=noise_map, model_data=model_data\n",
    ")\n",
    "plot_line(xvalues=xvalues, line=chi_squared_map, color=\"r\", ylabel=\"Chi-Squareds\")\n",
    "\n",
    "log_likelihood = log_likelihood_from_data_and_model_data(\n",
    "    data=data, noise_map=noise_map, model_data=model_data\n",
    ")\n",
    "print(\"Log Likelihood:\")\n",
    "print(log_likelihood)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Wrap Up__\n",
    "\n",
    "You can now perform model-fitting with **PyAutoFit**! All we have to do is guess lots of parameters, over and over and\n",
    "over again, until we hit a model with a high log_likelihood. Yay!\n",
    "\n",
    "Of course, you're probably thinking, is that really it? Should we really be guessing models to find the best-fit?\n",
    "\n",
    "Obviously, the answer is no. Imagine our model was more complex, that it had many more parameters than just 3.\n",
    "Our approach of guessing parameters won't work -- it could take days, maybe years, to find models with a high\n",
    "log likelihood, and how could you even be sure they ware the best-fit models? Maybe a set of parameters you never tried\n",
    "provide an even better fit?\n",
    "\n",
    "Of course, there is a much better way to perform model-fitting, and in the next sections we'll take you through how\n",
    "to do such fitting in **PyAutoFit**, by setting up our model with priors and performing a non-linear search.\n",
    "\n",
    "__Your Model__\n",
    "\n",
    "To end, its worth quickly again thinking about the model you ultimately want to fit to do your science. In this example,\n",
    "we extended the `Gaussian` class to contain the function we needed to generate an image of the `Gaussian` and thus\n",
    "generate the model-image we need to fit our data. For your model fitting problem can you do something similar?\n",
    "Or is your model-fitting task a bit more complicated than this? Maybe there are more model component you want to\n",
    "combine or there is an inter-dependency between models?\n",
    "\n",
    "Probabilistic programming languages provides a lot of flexibility in how you use your model instances, so whatever\n",
    "your problem you should find that it is straight forward to find a solution. But, whatever you need to do at its\n",
    "core your modeling problem will break down into the tasks we did in this tutorial:\n",
    "\n",
    "- Use your model to create some model data.\n",
    "- Subtract it from the data to create residuals.\n",
    "- Use these residuals in conjunction with your noise-map to define a log likelihood.\n",
    "- Find the highest log likelihood models.\n",
    "\n",
    "So, get thinking about how these steps would be performed for your model!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Section 3: Parameter Space And Priors\n",
    "=====================================\n",
    "\n",
    "In the previous section, we defined fitting functions that allowed us to create model-data using realizations of a\n",
    "1D `Gaussian` model and fit it to the data. We achieved a good fit, but only by guessing values of parameters. In the\n",
    "next two sections we are going to learn how to fit a model to data properly, which means we first need to define\n",
    "concepts of a parameter space and priors.\n",
    "\n",
    "__Parameter Space__\n",
    "\n",
    "If mathematics, you will have learnt that we can write a simple function as follows:\n",
    "\n",
    "$f(x) = x^2$\n",
    "\n",
    "In this function, when we input the parameter $x`$ in to the function $f$, it returns a value $f(x)$. The mappings\n",
    "between values of $x$ and $f(x)$ define what we can call the parameter space of this function (and if you remember\n",
    "your math classes, the parameter space of the function $f(x) = x^2$ is a parabola).\n",
    "\n",
    "A function can of course have multiple parameters:\n",
    "\n",
    "$f(x, y, z) = x + y^2 - z^3$\n",
    "\n",
    "This function has 3 parameters, $x$, $y$ and $z$. The mappings between $x$, $y$ and $z$ and $f(x, y, z)$ define another\n",
    "parameter space, albeit this parameter space now has 3 dimensions. Nevertheless, just like we could plot a parabola to\n",
    "visualize the parameter space $f(x) = x^2$, we could visualize this parameter space as 3 dimensional surface.\n",
    "\n",
    "In the previous tutorial, we used realizations of the `Gaussian` class to fit data with a model so as to return a log\n",
    "likelihood.\n",
    "\n",
    "This process can be thought of as us computing a likelihood from a function, just like our functions $f(x)$ above.\n",
    "However, the log likelihood function is not something that we can write down analytically as an equation and its\n",
    "behaviour is inherently non-linear. Nevertheless, it is a function, and if we put the same values of model\n",
    "parameters into this function the same value of log likelihood will be returned.\n",
    "\n",
    "Therefore, we can write this log likelihood function as follows, where the parameters $(x, N, \\sigma)$ are again\n",
    "the parameters of our `Gaussian`:\n",
    "\n",
    "$f(x, N, \\sigma) = log_likelihood$\n",
    "\n",
    "By expressing the likelihood in this way we have defined a parameter space! The solutions to this function cannot be\n",
    "written analytically and it is highly complex and non-linear. However, we have already learnt how we can use this\n",
    "function to determine a log likelihood, by creating realizations of the Gaussian and comparing them to the data.\n",
    "\n",
    "__Priors__\n",
    "\n",
    "We are now thinking about our model and log likelihood function as a parameter space, which is crucial for\n",
    "understanding how we will fit the model to data in the next tutorial. Before we do that, we need to consider one more\n",
    "concept, how do we define where in parameter space we search for solutions? What values of model parameters do we\n",
    "consider viable solutions?\n",
    "\n",
    "A parameter, say, the `centre` of the `Gaussian`, could in principle take any value between negative and positive\n",
    "infinity. However, when we inspect the data it is clearly confined to values between 0.0 and 100.0, therefore we should\n",
    "define a parameter space that only contains these solutions as these are the only physically plausible values\n",
    "of `centre` (e.g. between 0.0 --> 100.0).\n",
    "\n",
    "These are called the 'priors'. Our priors define where parameter space has valid solutions, and throughout these\n",
    "tutorials we will use three types of prior:\n",
    "\n",
    "- UniformPrior: The permitted values of a parameter are between a `lower_limit` and `upper_limit` and we assign equal\n",
    "probability to all solutions between these limits. For example, the `centre` of the `Gaussian` will typically assume\n",
    "a uniform prior between 0.0 and 100.0.\n",
    "\n",
    "- GaussianPrior: The permitted values of a parameter whose probability is tied to a Gaussian distribution with\n",
    "a `mean` and width `sigma`. For example, the `sigma` of the `Gaussian` will typically assume Gaussian prior with mean\n",
    "10.0 and sigma 5.0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Model Mapping via Priors__\n",
    "\n",
    "We can again use **PyAutoFit** to set the `Gaussian` as a model and map it to instances of the `Gaussian`, however\n",
    "we can now do this via priors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = af.Model(Gaussian)\n",
    "print(\"Model `Gaussian` object: \\n\")\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now set the prior for each parameter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
    "model.normalization = af.UniformPrior(lower_limit=1.0e-2, upper_limit=100.0)\n",
    "model.sigma = af.GaussianPrior(mean=10.0, sigma=5.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a quick reminder, we have seen that using this `Model` we can create an `instance` of the model, by mapping a\n",
    "list of physical values of each parameter as follows."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "instance = model.instance_from_vector(vector=[1.0, 2.0, 3.0])\n",
    "print(\"Instance Parameters \\n\")\n",
    "print(\"x = \", instance.centre)\n",
    "print(\"normalization = \", instance.normalization)\n",
    "print(\"sigma = \", instance.sigma)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Priors are used to create model instances via a mapping analogous to the one above, but from a unit-vector of values.\n",
    "\n",
    "This vector is defined in the same way as the vector above but with values spanning from 0 -> 1, where the unit values\n",
    "are mapped to physical values via the prior, for example:\n",
    "\n",
    "For the UniformPrior defined between 0.0 and 100.0:\n",
    "\n",
    "- An input unit value of 0.5 will give the physical value 5.0.\n",
    "- An input unit value of 0.8 will give te physical value 80.0.\n",
    "\n",
    "For a GaussianPrior defined with mean 10.0 and sigma 5.0:\n",
    "\n",
    "- An input unit value of 0.5 (e.g. the centre of the Gaussian) will give the physical value 10.0.\n",
    "- An input unit value of 0.8173 (e.g. 1 sigma confidence) will give te physical value 1.9051.\n",
    "\n",
    "Lets take a look:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "instance = model.instance_from_unit_vector(unit_vector=[0.5, 0.3, 0.8173])\n",
    "\n",
    "print(\"Model Instance: \\n\")\n",
    "print(instance)\n",
    "\n",
    "print(\"Instance Parameters \\n\")\n",
    "print(\"x = \", instance.centre)\n",
    "print(\"normalization = \", instance.normalization)\n",
    "print(\"sigma = \", instance.sigma)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__How Are Priors Actually Used?__\n",
    "\n",
    "Priors allow us to map unit vectors to physical parameters and therefore define a parameter space. However, the actual\n",
    "process of mapping unit-values to physical values in this way is pretty much all handled by **PyAutoFit** \n",
    "\"behind ths scenes\" and is not something you'll explicitly do yourself. Nevertheless, this is core concept of any\n",
    "model-fitting exercise and is why we have covered it in this tutorial. \n",
    "\n",
    "In the next section, we'll see how this mapping between unit and physical values is built-in to the algorithms we \n",
    "use to perform model-fitting!\n",
    "\n",
    "__Limits__\n",
    "\n",
    "We can also set physical limits on parameters, such that a model instance cannot generate parameters outside of a\n",
    "specified range.\n",
    "\n",
    "For example, a `Gaussian` cannot have a negative normalization, so we can set its lower limit to a value of 0.0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.normalization = af.GaussianPrior(\n",
    "    mean=0.0, sigma=1.0, lower_limit=0.0, upper_limit=1000.0\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The unit vector input below creates a negative normalization value, such that if you uncomment the line\n",
    "below **PyAutoFit** raises an error."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# instance = model.instance_from_unit_vector(unit_vector=[0.01, 0.01, 0.01])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Wrap Up__\n",
    "\n",
    "In this section, we introduce the notion of a parameter space and priors, which **PyAutoFit**'s model mapping\n",
    "utilities map between. We are now in a position to perform a model-fit, which will be the subject of the next section.\n",
    "\n",
    "The description of priors in this tutorial was somewhat of a simplification; we viewed them as a means to map a\n",
    "unit values of parameters to physical values. In Bayesian inference, priors play a far more important role, as they\n",
    "define one's previous knowledge of the model before performing the fit. They directly impact the solution that one\n",
    "infers and ultimately dictate how the model-fitting is performed.\n",
    "\n",
    "The aim of these lectures is not to teach the reader the details of Bayesian inference but instead set you up with the\n",
    "tools necessary to perform a model-fit. Nevertheless, it is worth reading up on Bayesian inference and priors at any of\n",
    "the following links:\n",
    "\n",
    "https://seeing-theory.brown.edu/bayesian-inference/index.html\n",
    "\n",
    "https://towardsdatascience.com/probability-concepts-explained-bayesian-inference-for-parameter-estimation-90e8930e5348"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Section 4: Non-linear Search\n",
    "============================\n",
    "\n",
    "Its finally time to take our model and fit it to data.\n",
    "\n",
    "So, how do we infer the parameters for the 1D `Gaussian` that give a good fit to our data?  Previously, we tried a very\n",
    "basic approach, randomly guessing models until we found one that gave a good fit and high log_likelihood. We discussed\n",
    "that this was not a viable strategy for more complex models. Surprisingly, this is the basis of how model fitting\n",
    "actually works!\n",
    "\n",
    "Basically, our model-fitting algorithm guesses lots of models, tracking the log likelihood of these models. As the\n",
    "algorithm progresses, it begins to guess more models using parameter combinations that gave higher log_likelihood\n",
    "solutions previously. If a set of parameters provided a good fit to the data previously, a model with similar values\n",
    "probably will too.\n",
    "\n",
    "This is called a non-linear search, and is where the notion of a \"parameter space\" comes in. We are essentially\n",
    "searching the parameter space defined by the log likelihood function we introduced in the previous tutorial. Why is it\n",
    "called non-linear? Because this function is non-linear.\n",
    "\n",
    "Non linear searches are a common tool used by scientists in a wide range of fields. We will use a non-linear search\n",
    "algorithm called `Emcee`, which for those familiar with statistic inference is a Markov Chain Monte Carlo (MCMC)\n",
    "method. For now, lets not worry about the details of how Emcee actually works. Instead, just picture that a non-linear\n",
    "search in **PyAutoFit** operates as follows:\n",
    "\n",
    " 1) Randomly guess a model and map the parameters via the priors to an instance of the model, in this case\n",
    " our `Gaussian`.\n",
    "\n",
    " 2) Use this model instance to generate model data and compare this model data to the data to compute a log likelihood.\n",
    "\n",
    " 3) Repeat this many times, choosing models whose parameter values are near those of models which have higher log\n",
    " likelihood values. If a new model's log likelihood is higher than previous models, new models will be chosen with\n",
    " parameters nearer this model.\n",
    "\n",
    "The idea is that if we keep guessing models with higher log-likelihood values, we will inevitably `climb` up the\n",
    "gradient of the log likelihood in parameter space until we eventually hit the highest log likelihood models.\n",
    "\n",
    "To be clear, this overly simplified description of an MCMC algorithm is not how `Emcee` actually works in detail. We\n",
    "are omitting crucial details on how our priors impact our inference as well as how the MCMC algorithm provides us with\n",
    "reliable errors on our parameter estimates. The goal of this chapter to teach you how to use **PyAutoFit**, not the\n",
    "actual details of Bayesian inference. If you are interested in the details of how MCMC works, I recommend you checkout\n",
    "the following web links:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo\n",
    "\n",
    "https://twiecki.io/blog/2015/11/10/mcmc-sampling/\n",
    "\n",
    "https://towardsdatascience.com/a-zero-math-introduction-to-markov-chain-monte-carlo-methods-dcba889e0c50\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets remind ourselves what the data looks like, using defining a `plot_line` method for convenience.\n",
    "\n",
    "Note that this function has tools for outputting the images to hard-disk as `.png` files, which we'll use later\n",
    "in this section."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_line(\n",
    "    xvalues,\n",
    "    line,\n",
    "    title=None,\n",
    "    ylabel=None,\n",
    "    errors=None,\n",
    "    color=\"k\",\n",
    "    output_path=None,\n",
    "    output_filename=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a 1D line of data on a plot of x versus y, where the x-axis is the x coordinate of the line and the y-axis\n",
    "    is the normalization of the line at that coordinate.\n",
    "\n",
    "    The function include options to output the image to the hard-disk as a .png.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xvalues : np.ndarray\n",
    "        The x-coordinates the profile is defined on.\n",
    "    line : np.ndarray\n",
    "        The normalization values of the profile which are plotted.\n",
    "    ylabel : str\n",
    "        The y-label of the plot.\n",
    "    output_path : str\n",
    "        The path the image is to be output to hard-disk as a .png.\n",
    "    output_filename : str\n",
    "        The filename of the file if it is output as a .png.\n",
    "    output_format : str\n",
    "        Determines where the plot is displayed on your screen (\"show\") or output to the hard-disk as a png (\"png\").\n",
    "    \"\"\"\n",
    "    plt.errorbar(\n",
    "        x=xvalues, y=line, yerr=errors, color=color, ecolor=\"k\", elinewidth=1, capsize=2\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x value of profile\")\n",
    "    plt.ylabel(ylabel)\n",
    "    if output_filename is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        if not path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "        plt.savefig(path.join(output_path, f\"{output_filename}.png\"))\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "xvalues = np.arange(data.shape[0])\n",
    "\n",
    "plot_line(xvalues=xvalues, line=data, errors=noise_map, title=\"Data\", ylabel=\"Data\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Analysis__\n",
    "\n",
    "The non-linear search requires an `Analysis` class, which:\n",
    "\n",
    " - Receives the data to be fitted and prepares it so the model can fit it.\n",
    "\n",
    " - Defines the `log_likelihood_function` used to compute the `log_likelihood` from a model instance.\n",
    "\n",
    " - Passes this `log_likelihood` to the non-linear search so that it can determine parameter values for the the next\n",
    " model that it samples.\n",
    "\n",
    "For our 1D `Gaussian` model-fitting example, here is our `Analysis` class (read the comment in \n",
    "the `log_likelihood_function` for a description of how model mapping is used to set up the model that each iteration\n",
    "of the non-linear search fits):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Analysis(af.Analysis):\n",
    "    def __init__(self, data, noise_map):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.noise_map = noise_map\n",
    "\n",
    "    def log_likelihood_function(self, instance):\n",
    "\n",
    "        \"\"\"\n",
    "        The `instance` that comes into this method is an instance of the `Gaussian` class above, with the parameters\n",
    "        set to values chosen by the non-linear search (These are commented out to prevent excessive print statements\n",
    "        when we run the non-linear search).\n",
    "\n",
    "        This instance`s parameter values are chosen by the non-linear search based on the priors of each parameter and\n",
    "        the previous models with the highest likelihood result. They are set up as physical values, by mapping unit\n",
    "        values chosen by the non-linear search.\n",
    "\n",
    "        print(\"Gaussian Instance:\")\n",
    "        print(\"Centre = \", instance.centre)\n",
    "        print(\"Normalization = \", instance.normalization)\n",
    "        print(\"Sigma = \", instance.sigma)\n",
    "\n",
    "        Below, we fit the data with the `Gaussian` instance, using its \"profile_from_xvalues\" function to create the\n",
    "        model data.\n",
    "        \"\"\"\n",
    "        xvalues = np.arange(self.data.shape[0])\n",
    "\n",
    "        model_data = instance.profile_from_xvalues(xvalues=xvalues)\n",
    "        residual_map = self.data - model_data\n",
    "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
    "        chi_squared = sum(chi_squared_map)\n",
    "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map ** 2.0))\n",
    "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
    "\n",
    "        return log_likelihood\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Visualization__\n",
    "\n",
    "The `Analysis` class above is all we need to fit our model to data with a non-linear search. However, it will provide\n",
    "us with limited output to inspect whether the fit was a success or not\n",
    "\n",
    "By extending the `Analysis` class with a `visualize` function, we can perform on-the-fly visualization, which outputs\n",
    "images of the quantities we described in tutorial 2 to hard-disk as `.png` files using the `plot_line` function above.\n",
    "\n",
    "Visualization of the results of the search, such as the corner plot of what is caleld the \"Probability Density \n",
    "Function\", are also automatically output during the model-fit on the fly."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class Analysis(af.Analysis):\n",
    "    def __init__(self, data, noise_map):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.noise_map = noise_map\n",
    "\n",
    "    def log_likelihood_function(self, instance):\n",
    "\n",
    "        \"\"\"\n",
    "        The `log_likelihood_function` is identical to the previous tutorial.\n",
    "        \"\"\"\n",
    "        xvalues = np.arange(self.data.shape[0])\n",
    "\n",
    "        model_data = instance.profile_from_xvalues(xvalues=xvalues)\n",
    "        residual_map = self.data - model_data\n",
    "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
    "        chi_squared = sum(chi_squared_map)\n",
    "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map ** 2.0))\n",
    "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
    "\n",
    "        return log_likelihood\n",
    "\n",
    "    def visualize(self, paths, instance, during_analysis):\n",
    "\n",
    "        \"\"\"\n",
    "        During a model-fit, the `visualize` method is called throughout the non-linear search. The `instance` passed\n",
    "        into the visualize method is maximum log likelihood solution obtained by the model-fit so far and it can be\n",
    "        used to provide on-the-fly images showing how the model-fit is going.\n",
    "        \"\"\"\n",
    "        xvalues = np.arange(self.data.shape[0])\n",
    "\n",
    "        model_data = instance.profile_from_xvalues(xvalues=xvalues)\n",
    "        residual_map = self.data - model_data\n",
    "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
    "\n",
    "        \"\"\"\n",
    "        The visualizer now outputs images of the best-fit results to hard-disk (checkout `visualizer.py`).\n",
    "        \"\"\"\n",
    "        plot_line(\n",
    "            xvalues=xvalues,\n",
    "            line=self.data,\n",
    "            title=\"Data\",\n",
    "            ylabel=\"Data Values\",\n",
    "            color=\"k\",\n",
    "            output_path=paths.image_path,\n",
    "            output_filename=\"data\",\n",
    "        )\n",
    "\n",
    "        plot_line(\n",
    "            xvalues=xvalues,\n",
    "            line=model_data,\n",
    "            title=\"Model Data\",\n",
    "            ylabel=\"Model Data Values\",\n",
    "            color=\"k\",\n",
    "            output_path=paths.image_path,\n",
    "            output_filename=\"model_data\",\n",
    "        )\n",
    "\n",
    "        plot_line(\n",
    "            xvalues=xvalues,\n",
    "            line=residual_map,\n",
    "            title=\"Residual Map\",\n",
    "            ylabel=\"Residuals\",\n",
    "            color=\"k\",\n",
    "            output_path=paths.image_path,\n",
    "            output_filename=\"residual_map\",\n",
    "        )\n",
    "\n",
    "        plot_line(\n",
    "            xvalues=xvalues,\n",
    "            line=chi_squared_map,\n",
    "            title=\"Chi-Squared Map\",\n",
    "            ylabel=\"Chi-Squareds\",\n",
    "            color=\"k\",\n",
    "            output_path=paths.image_path,\n",
    "            output_filename=\"chi_squared_map\",\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Search__\n",
    "\n",
    "To perform the non-linear search using `Emcee`, we simply compose our model using a `Model`, instantiate the\n",
    "`Analysis` class and pass them to an instance of the `Emcee` class.\n",
    "\n",
    "We also pass a `name` and `path_prefrix`, which specifies that when the results are output to the folder\n",
    "`autofit_workspace/output` they'll also be written to the folder `howtofit/chapter_1/section_non_linear_search`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = af.Model(Gaussian)\n",
    "\n",
    "model.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
    "model.normalization = af.UniformPrior(lower_limit=1.0e-2, upper_limit=1.0e2)\n",
    "model.sigma = af.GaussianPrior(mean=10.0, sigma=5.0, lower_limit=0.0, upper_limit=np.inf)\n",
    "\n",
    "analysis = Analysis(data=data, noise_map=noise_map)\n",
    "\n",
    "emcee = af.Emcee(\n",
    "    path_prefix=path.join(\"howtofit\", \"chapter_1\"), name=\"section_non_linear_search\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Model Fit__\n",
    "\n",
    "We begin the non-linear search by calling its `fit` method. This will take a minute or so to run (which is very fast\n",
    "for a model-fit). Whilst you're waiting, checkout the folder:\n",
    "\n",
    "`autofit_workspace/output/howtofit`\n",
    "\n",
    "Here, the results of the model-fit are output to your hard-disk (on-the-fly) and you can inspect them as the non-linear\n",
    "search runs.\n",
    "\n",
    "__Unique Identifier__\n",
    "\n",
    "In the output folder, you will note that results are in a folder which is a collection of random characters. This acts\n",
    "as a `unique_identifier` of the model-fit, where this identifier is generated based on the model, priors and search that\n",
    "are used in the fit.\n",
    "\n",
    "An identical combination of model and search generates the same identifier, meaning that rerunning the\n",
    "script will use the existing results to resume the model-fit. In contrast, if you change the model, priors or search,\n",
    "a new unique identifier will be generated, ensuring that the model-fit results are output into a separate folder.\n",
    "\n",
    "__Contents__\n",
    "\n",
    "In particular, you'll find (in a folder that is a random string of characters):\n",
    "\n",
    " - `model.info`: A file listing every model component, parameter and prior in your model-fit.\n",
    "\n",
    " - `model.results`: A file giving the latest best-fit model, parameter estimates and errors of the fit.\n",
    "\n",
    " - `output.log`: A file containing the text output of the non-linear search.\n",
    "\n",
    " - `samples`: A folder containing the `Emcee` output in hdf5 format.txt (you'll probably never need to look at these,\n",
    "   but its good to know what they are).\n",
    "\n",
    " - `search.summary` A file containing information on the search, including the total number of samples,\n",
    " overall run-time and time it takes to evaluate the log likelihood function.\n",
    "\n",
    " - `image`: A folder containing `.png` files of the fits defined in the `visualize` method.\n",
    "\n",
    " - Other metadata which you can ignore for now (e.g. the pickles folder)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = emcee.fit(model=model, analysis=analysis)\n",
    "\n",
    "print(\n",
    "    \"Emcee has begun running - checkout the autofit_workspace/output/howtofit/section_non_linear_search\"\n",
    "    \" folder for live output of the results.\"\n",
    "    \"This Jupyter notebook cell with progress once Emcee has completed - this could take a few minutes!\"\n",
    ")\n",
    "\n",
    "print(\"Emcee has finished run - you may now continue the notebook.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Result__\n",
    "\n",
    "Once completed, the non-linear search returns a `Result` object, which contains lots of information about the\n",
    "NonLinearSearch.\n",
    "\n",
    "A full description of the `Results` object can be found on the **PyAutoFit** readthedocs.\n",
    "\n",
    "Lets use the `result` it to inspect the maximum likelihood model instance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Maximum Likelihood Model:\\n\")\n",
    "max_log_likelihood_instance = result.samples.max_log_likelihood_instance\n",
    "print(\"Centre = \", max_log_likelihood_instance.centre)\n",
    "print(\"Normalization = \", max_log_likelihood_instance.normalization)\n",
    "print(\"Sigma = \", max_log_likelihood_instance.sigma)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use this to plot the maximum log likelihood fit over the data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_data = result.max_log_likelihood_instance.profile_from_xvalues(\n",
    "    xvalues=np.arange(data.shape[0])\n",
    ")\n",
    "plt.errorbar(\n",
    "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
    ")\n",
    "plt.plot(xvalues, model_data, color=\"r\")\n",
    "plt.title(\"Emcee model fit to 1D Gaussian dataset.\")\n",
    "plt.xlabel(\"x values of profile\")\n",
    "plt.ylabel(\"Profile normalization\")\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Samples__\n",
    "\n",
    "Above, we used the `Result`'s `samples` property, which in this case is a `MCMCSamples` object:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(result.samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This object acts as an interface between the `Emcee` output results on your hard-disk and this Python code. For\n",
    "example, we can use it to get the parameters and log likelihood of an accepted emcee sample."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(result.samples.parameter_lists[10][:])\n",
    "print(result.samples.log_likelihood_list[10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also use it to get a model instance of the `median_pdf` model, which is the model where each parameter is\n",
    "the value estimated from the probability distribution of parameter space."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mp_instance = result.samples.median_pdf_instance\n",
    "print()\n",
    "print(\"Median PDF Model:\\n\")\n",
    "print(\"Centre = \", mp_instance.centre)\n",
    "print(\"Normalization = \", mp_instance.normalization)\n",
    "print(\"Sigma = \", mp_instance.sigma)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Probability Density Functions (PDF's) of the results can be plotted using the Emcee's visualization\n",
    "tool `corner.py`, which is wrapped via the `EmceePlotter` object.\n",
    "\n",
    "The PDF shows the 1D and 2D probabilities estimated for every parameter after the model-fit. The two dimensional\n",
    "figures can show the degeneracies between different parameters, for example how increasing $\\sigma$ and decreasing\n",
    "the normalization $I$ can lead to similar likelihoods and probabilities."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import autofit.plot as aplt\n",
    "\n",
    "emcee_plotter = aplt.EmceePlotter(samples=result.samples)\n",
    "emcee_plotter.corner()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The PDF figure above can be seen to have labels for all parameters, whereby sigma appears as a sigma symbol, the\n",
    "normalization is `N`, and centre is `x`. This is set via the config file `config/notation/label.ini`. When you write\n",
    "your own model-fitting code with **PyAutoFit**, you can update this config file so your PDF's automatically have the\n",
    "correct labels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Section 5: Complex Models\n",
    "=========================\n",
    "\n",
    "Up to now, we've fitted a very simple model, a 1D `Gaussian` with 3 free parameters. In this tutorial, we'll look at\n",
    "how **PyAutoFit** allows us to compose and fit models of arbitrary complexity.\n",
    "\n",
    "To begin, you should check out the module `autofit_workspace/howtofit/chapter_1_introduction/profiles.py`.\n",
    "\n",
    "In previous tutorials we used the module `gaussian.py` which contained only the `Gaussian` class. The `profiles.py`\n",
    "includes a second profile, `Exponential`, which like the `Gaussian` class is a model-component that can be fitted to\n",
    "data.\n",
    "\n",
    "Up to now, our data has always been generated using a single `Gaussian` profile. Thus, we have only needed to fit\n",
    "it with a single `Gaussian`. In this tutorial, our `dataset` is now a superpositions of multiple profiles. The models\n",
    "we compose and fit are therefore composed of multiple profiles, such that when we generate the model-data we\n",
    "generate it as the sum of all individual profiles in our model.\n",
    "\n",
    "Defining a model using multiple model components is straight forward in **PyAutoFit**, using a `Collection`\n",
    "object."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = af.Collection(\n",
    "    gaussian_0=af.Model(Gaussian), gaussian_1=af.Model(Gaussian)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A `Collection` behaves like a `Model` but contains a collection of model components. For example, it\n",
    "creates a model instance by mapping a list of parameters, which in this case is 6 (3 for the `Gaussian` (centre,\n",
    "normalization, sigma) and 3 for the `Exponential` (centre, normalization, rate))."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "instance = model.instance_from_vector(vector=[0.1, 0.2, 0.3, 0.4, 0.5, 0.01])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This `instance` contains each of the model components we defined above, using the input argument name of the\n",
    "`Collection` to define the attributes in the `instance`:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Instance Parameters \\n\")\n",
    "print(\"x (Gaussian 0) = \", instance.gaussian_0.centre)\n",
    "print(\"normalization (Gaussian 0) = \", instance.gaussian_0.normalization)\n",
    "print(\"sigma (Gaussian 0) = \", instance.gaussian_0.sigma)\n",
    "print(\"x (Gaussian 1) = \", instance.gaussian_1.centre)\n",
    "print(\"normalization (Gaussian 1) = \", instance.gaussian_1.normalization)\n",
    "print(\"sigma (Gaussian 1) = \", instance.gaussian_1.sigma)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using `Collections`'s we could now define an `Analysis` class that allows us to fit models with as many `Gaussians`\n",
    "in the model as we like, alongside other 1D profiles like a symmetric `Exponential`. The **PyAutoFit** docs have\n",
    "example scripts and tutorials showing how to do this; but for this workshop we'll instead jump to the Astronomy\n",
    "use case in lecture 2."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}